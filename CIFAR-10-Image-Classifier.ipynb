{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "UhSzBqvXk_NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall tf-keras\n",
        "# !pip install keras-tuner\n",
        "# !pip install tensorflow==2.16.1"
      ],
      "metadata": {
        "id": "CohUdXdZ_3us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import tensorflow as tf\n",
        "print(\"Keras Current Version:\", keras.__version__, \"Tensorflow Current Version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xg0IL3q7fOk",
        "outputId": "792fc723-a3aa-4a70-c1e8-0173d3c22d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keras Current Version: 3.8.0 Tensorflow Current Version: 2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "4fkaU13ZlJps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wRAN0T15pcE",
        "outputId": "c5bdabfb-f700-440e-a1d9-d15aee831dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.4.26)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.13.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras_tuner as kt"
      ],
      "metadata": {
        "id": "jKr_Vp_Z9gaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from joblib import dump, load\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.initializers import RandomNormal, RandomUniform, GlorotUniform, GlorotNormal, HeNormal\n",
        "from keras.optimizers.schedules import ExponentialDecay\n",
        "from keras_tuner import RandomSearch, GridSearch, BayesianOptimization\n",
        "from keras_tuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "random.seed(46)\n",
        "np.random.seed(46)\n",
        "tf.random.set_seed(46)\n",
        "\n",
        "# import os\n",
        "import time\n"
      ],
      "metadata": {
        "id": "a5JXY1-PidHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "AGxuS3MVhyHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(filepath):\n",
        "    data = pd.read_csv(filepath)\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(data.drop('Outcome', axis=1))\n",
        "    y = data['Outcome'].values\n",
        "    dump(scaler, 'scaler.joblib')\n",
        "    return X, y\n",
        "\n",
        "def prepare_datasets(X_train, X_val, y_train, y_val, batch_size=None):\n",
        "    if batch_size is None:\n",
        "        batch_size = len(X_train)\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size=len(X_train)).batch(batch_size)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
        "    val_dataset = val_dataset.batch(batch_size)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "def plot_training_history(history, train_loss='loss', train_metric='accuracy', val_loss='val_loss', val_metric='val_accuracy'):\n",
        "\n",
        "    #Loss\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history[train_loss], label='Training Loss')\n",
        "    plt.plot(history.history[val_loss], label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Metrics\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(history.history[train_metric], label=f\"Training: {train_metric}\")\n",
        "    plt.plot(history.history[val_metric], label=f\"Validation: {val_metric}\")\n",
        "    plt.title(f'Training and Validation {train_metric} Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(f'train_metric')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def get_best_epoch_details(history):\n",
        "    val_losses = history.history['val_loss']\n",
        "    min_val_loss_index = val_losses.index(min(val_losses))\n",
        "    best_epoch = min_val_loss_index + 1\n",
        "\n",
        "    epoch_details = {}\n",
        "    for key in history.history.keys():\n",
        "        epoch_details[key] = history.history[key][min_val_loss_index]\n",
        "\n",
        "    epoch_details['best_epoch'] = best_epoch\n",
        "    print(f\"Best epoch details: {epoch_details}\")"
      ],
      "metadata": {
        "id": "qmBVpPJ0hz33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "GQcoZeUPhjEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = preprocess_data('diabetes.csv')\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_ds, val_ds = prepare_datasets(X_train, X_val, y_train, y_val, batch_size=32)"
      ],
      "metadata": {
        "id": "o9V6zzvdgwUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(train_ds.element_spec[0].shape[1],)))\n",
        "\n",
        "    # Hidden layers, activation functions, l2, Dropout\n",
        "    for i in range(hp.Int(\"num_layers\", min_value=1, max_value=10)): # 1-10 layer\n",
        "\n",
        "        model.add(Dense(units=hp.Int(f\"units{i}\", min_value=32, max_value=512, step=16), # 2-512 arasında 16'şar\n",
        "                        activation=hp.Choice('activation_' + str(i), values=[\"relu\", \"sigmoid\", \"tanh\"]), # Aktivasyon fonksiyonları relu, tanh, sigmoid\n",
        "                        kernel_regularizer=l2(hp.Float(f\"l2_{i}\", min_value=1e-4, max_value=1e-2, sampling=\"log\")))) #  0.0001-0.01\n",
        "\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Dropout(hp.Float(f\"dropout{i}\", min_value=0.1, max_value=0.5, step = 0.05))) # 0.1-0.5 arasında 0.05\n",
        "\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Learning rate schedule\n",
        "    initial_learning_rate = hp.Float(\"initial_learning_rate\", min_value = 1e-4, max_value = 1e-2, sampling=\"log\") # 0.0001-0.01 (1e-4 - 1e-2)\n",
        "\n",
        "    lr_schedule = ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps = 20, #  20\n",
        "        decay_rate=0.96,\n",
        "        staircase=True\n",
        "    )\n",
        "\n",
        "    # optimizers\n",
        "    optimizer_choice = hp.Choice('optimizer', values=['sgd', 'adam', \"rmsprop\"])\n",
        "    if optimizer_choice == 'sgd':\n",
        "        optimizer = SGD(\n",
        "            learning_rate=lr_schedule,\n",
        "            momentum=hp.Float('momentum', min_value=0.0, max_value=0.9, step=0.1)\n",
        "        )\n",
        "    elif optimizer_choice == 'adam':\n",
        "        optimizer = Adam(\n",
        "            learning_rate=lr_schedule,\n",
        "            beta_1=hp.Float('beta1', min_value=0.85, max_value=0.99, step=0.01),\n",
        "            beta_2=hp.Float('beta2', min_value=0.999, max_value=0.9999, step=0.0001),\n",
        "            epsilon=hp.Float('epsilon', min_value=1e-8, max_value=1e-7, step=1e-8)\n",
        "        )\n",
        "\n",
        "    elif optimizer_choice == 'rmsprop':\n",
        "        optimizer = RMSprop(\n",
        "            learning_rate=lr_schedule,\n",
        "            rho=hp.Float('rho', min_value=0.8, max_value=0.99, step=0.01),\n",
        "            epsilon=hp.Float('epsilon', min_value=1e-10, max_value=1e-8, step=1e-10),\n",
        "            momentum=hp.Float('momentum', min_value=0.0, max_value=0.9, step=0.1)\n",
        "        )\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "P9qovA7mtTNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tunner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective = \"val_loss\",\n",
        "    max_trials = 50,\n",
        "    directory = \"kt_random\",\n",
        "    project_name= \"diabets\",\n",
        "    overwrite = True\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "tunner.search(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=200,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "XYEI8pJbs2GA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9334d5-9f53-4869-b16c-d7d75056f239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 50 Complete [00h 00m 19s]\n",
            "val_loss: 0.6639091968536377\n",
            "\n",
            "Best val_loss So Far: 0.5059324502944946\n",
            "Total elapsed time: 00h 22m 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_3 = tunner.get_best_hyperparameters(num_trials=3)"
      ],
      "metadata": {
        "id": "lgSUmFVys2Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set 1 için öğreneğin  başlangıç activation function ı sigmoid ve sonra relu tanh karışık şekilde  gitmiş en son çıktı katmanında da yine tanh kullanılmış . ilk katmanda 416 nöron varmış. en son katmanda da 400 nöron varmış."
      ],
      "metadata": {
        "id": "t-jQPtPpyTyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import json, os\n",
        "os.makedirs(\"best_models\", exist_ok=True)\n",
        "\n",
        "for i, hp in enumerate(top_3, start=1):\n",
        "    with open(f\"best_models/hp_{i}.json\", \"w\") as f:\n",
        "        json.dump(hp.values, f, indent=2)\n",
        "\n",
        "    print(f\"\\n––– Hyper-parameter set #{i} –––\")\n",
        "    for k, v in hp.values.items():\n",
        "        print(f\"{k:>18}: {v}\")\n"
      ],
      "metadata": {
        "id": "ctXttysPH2wv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e94224-33b2-48ca-f799-c740e4e08e47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "––– Hyper-parameter set #1 –––\n",
            "        num_layers: 1\n",
            "            units0: 416\n",
            "      activation_0: sigmoid\n",
            "              l2_0: 0.0010339633001967396\n",
            "          dropout0: 0.2\n",
            "initial_learning_rate: 0.004256409192507239\n",
            "         optimizer: rmsprop\n",
            "          momentum: 0.2\n",
            "            units1: 64\n",
            "      activation_1: relu\n",
            "              l2_1: 0.00021091169566063192\n",
            "          dropout1: 0.4\n",
            "            units2: 208\n",
            "      activation_2: tanh\n",
            "              l2_2: 0.00023732836845124968\n",
            "          dropout2: 0.30000000000000004\n",
            "            units3: 144\n",
            "      activation_3: relu\n",
            "              l2_3: 0.00017168785604646604\n",
            "          dropout3: 0.45000000000000007\n",
            "            units4: 32\n",
            "      activation_4: relu\n",
            "              l2_4: 0.0010748080614242867\n",
            "          dropout4: 0.2\n",
            "            units5: 384\n",
            "      activation_5: tanh\n",
            "              l2_5: 0.00011687281140446384\n",
            "          dropout5: 0.35\n",
            "            units6: 176\n",
            "      activation_6: tanh\n",
            "              l2_6: 0.0002875309185845335\n",
            "          dropout6: 0.30000000000000004\n",
            "            units7: 496\n",
            "      activation_7: tanh\n",
            "              l2_7: 0.0005485114093661166\n",
            "          dropout7: 0.1\n",
            "               rho: 0.8400000000000001\n",
            "           epsilon: 9.6e-09\n",
            "             beta1: 0.87\n",
            "             beta2: 0.9997\n",
            "            units8: 400\n",
            "      activation_8: tanh\n",
            "              l2_8: 0.0001936313754705854\n",
            "          dropout8: 0.4\n",
            "\n",
            "––– Hyper-parameter set #2 –––\n",
            "        num_layers: 1\n",
            "            units0: 400\n",
            "      activation_0: tanh\n",
            "              l2_0: 0.0007022578158084675\n",
            "          dropout0: 0.1\n",
            "initial_learning_rate: 0.0005423774770280414\n",
            "         optimizer: rmsprop\n",
            "          momentum: 0.1\n",
            "            units1: 256\n",
            "      activation_1: relu\n",
            "              l2_1: 0.0001521873437549712\n",
            "          dropout1: 0.35\n",
            "            units2: 480\n",
            "      activation_2: relu\n",
            "              l2_2: 0.002488466363710587\n",
            "          dropout2: 0.1\n",
            "            units3: 272\n",
            "      activation_3: relu\n",
            "              l2_3: 0.007321002854350309\n",
            "          dropout3: 0.4\n",
            "            units4: 368\n",
            "      activation_4: sigmoid\n",
            "              l2_4: 0.0009031111745904648\n",
            "          dropout4: 0.30000000000000004\n",
            "            units5: 448\n",
            "      activation_5: sigmoid\n",
            "              l2_5: 0.00025578735394703227\n",
            "          dropout5: 0.2\n",
            "            units6: 432\n",
            "      activation_6: sigmoid\n",
            "              l2_6: 0.0022124410649827704\n",
            "          dropout6: 0.1\n",
            "            units7: 240\n",
            "      activation_7: sigmoid\n",
            "              l2_7: 0.0006635477373656421\n",
            "          dropout7: 0.25\n",
            "               rho: 0.9400000000000001\n",
            "           epsilon: 7.7e-09\n",
            "             beta1: 0.85\n",
            "             beta2: 0.9997\n",
            "            units8: 64\n",
            "      activation_8: relu\n",
            "              l2_8: 0.0006018043624072696\n",
            "          dropout8: 0.5\n",
            "            units9: 352\n",
            "      activation_9: sigmoid\n",
            "              l2_9: 0.0044008493021638305\n",
            "          dropout9: 0.4\n",
            "\n",
            "––– Hyper-parameter set #3 –––\n",
            "        num_layers: 1\n",
            "            units0: 432\n",
            "      activation_0: sigmoid\n",
            "              l2_0: 0.001698498750487087\n",
            "          dropout0: 0.1\n",
            "initial_learning_rate: 0.006857612343716452\n",
            "         optimizer: adam\n",
            "          momentum: 0.6000000000000001\n",
            "            units1: 336\n",
            "      activation_1: relu\n",
            "              l2_1: 0.0008979340534482525\n",
            "          dropout1: 0.45000000000000007\n",
            "            units2: 432\n",
            "      activation_2: tanh\n",
            "              l2_2: 0.00014014488528467923\n",
            "          dropout2: 0.30000000000000004\n",
            "            units3: 160\n",
            "      activation_3: sigmoid\n",
            "              l2_3: 0.000737956160144406\n",
            "          dropout3: 0.35\n",
            "            units4: 256\n",
            "      activation_4: relu\n",
            "              l2_4: 0.00013529433918186214\n",
            "          dropout4: 0.15000000000000002\n",
            "            units5: 80\n",
            "      activation_5: sigmoid\n",
            "              l2_5: 0.0022552054227376684\n",
            "          dropout5: 0.25\n",
            "            units6: 304\n",
            "      activation_6: sigmoid\n",
            "              l2_6: 0.0005110141491297823\n",
            "          dropout6: 0.2\n",
            "            units7: 448\n",
            "      activation_7: sigmoid\n",
            "              l2_7: 0.0026776510063065816\n",
            "          dropout7: 0.2\n",
            "               rho: 0.9400000000000001\n",
            "           epsilon: 6.7000000000000004e-09\n",
            "             beta1: 0.9\n",
            "             beta2: 0.9998\n",
            "            units8: 128\n",
            "      activation_8: tanh\n",
            "              l2_8: 0.00023824393969777363\n",
            "          dropout8: 0.4\n",
            "            units9: 368\n",
            "      activation_9: tanh\n",
            "              l2_9: 0.000328831566908209\n",
            "          dropout9: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tunner.get_best_models(num_models=3)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b3947zzTNT3X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8493bf-4e6a-4abe-a58b-f26fa1e3bff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 1 variables whereas the saved optimizer has 13 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 1 variables whereas the saved optimizer has 13 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, model in enumerate(best_model, start=1):\n",
        "    model.save(f\"best_models/model_{i}.h5\")"
      ],
      "metadata": {
        "id": "_JwgYB84yKtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b157b848-2f1f-47bf-aacf-ede99a5d6a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XsoQ_lfwxdC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nValidation performance:\")\n",
        "for i, model in enumerate(best_model, start=1):\n",
        "    loss, acc = model.evaluate(val_ds, verbose=0)\n",
        "    print(f\"Model {i}:  val_loss = {loss:.4f}   |   val_acc = {acc:.4f}\")"
      ],
      "metadata": {
        "id": "0CtBdY-YOpEq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7a6f25-ee51-4767-98cc-f144c2066e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation performance:\n",
            "Model 1:  val_loss = 0.5059   |   val_acc = 0.7727\n",
            "Model 2:  val_loss = 0.5078   |   val_acc = 0.8052\n",
            "Model 3:  val_loss = 0.5223   |   val_acc = 0.7662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Val _loss a göre sıralandığı için ve en düşük kayıp da her zaman en yüksek accuracy anlamına gelmediği için."
      ],
      "metadata": {
        "id": "7VYUsSMPxfiQ"
      }
    }
  ]
}